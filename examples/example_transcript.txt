 Hello everyone and welcome to another episode of Code Emporium where we're going to talk about temporal difference learning It's a very important concept in reinforcement learning. So let's get started. Let's start with the definition temporal difference learning is a method that value-based reinforcement learning algorithms like Q learning Use to iteratively learn state value functions or state action value functions Now every word in this definition is super strange. So let's back up temporal difference learning is a method to learn Okay, that's a little easier now. We're gonna add some more complexity to it temporal difference learning is a method reinforcement learning algorithms use to learn So what are reinforcement learning algorithms? reinforcement learning is one of the three machine learning paradigms that machines use to learn The first is supervised learning where we have some data and we have a label and we use that to train a model to map this data to the label Then that's typically like classification and regression problems Then we have unsupervised learning where we try to understand patterns within the data and there's data But there's no label example dimensionality reduction and clustering and then we have the third pillar which is reinforcement learning and Reinforcement learning is learning what to do that is how to map situations to actions So as to maximize a numerical reward signal So it maps states to actions to maximize a reward Algorithms that solve problems in this way are known as reinforcement learning algorithms and Here's a snapshot of a few pretty common ones coming back to this definition I hope this reinforcement learning algorithms component now makes more sense So let's peel off another layer Temporal difference learning is a method that value-based reinforcement learning algorithms used to learn so what is this value-based reinforcement learning algorithm? Reinforcement learning algorithms at the end of the day are trying to maximize a total reward This can happen in multiple ways and so we can subcategorize Reinforce me learning algorithms into value-based methods or policy-based methods Value-based methods will determine a value function which quantifies the total reward and using this value function We will determine an optimal policy Whereas policy-based methods will determine an optimal policy directly no value function and Optimal policy is that policy that maximizes the total reward and just to note policy is just how an agent behaves in a certain situation or state so given eight state what action will it take that's what the policy determines Example of value-based functions is Q learning among others and example of policy-based methods is proximal policy optimization among others So now coming back to our definition. I hope this value-based reinforcement learning term makes a little more sense Now let's peel off another layer Temporal difference learning is a method that value-based reinforcement learning algorithms Life Q learning used to learn state value functions or state action value functions So what are these state and state action value functions now? So value-based methods determine a value function which is used in turn to determine the policy that will maximize the total reward Value function is a function and it has inputs and it gives some output and depending on the inputs There can be two types of value functions. We have a state value function the input is a state And then we have a state action value function where the input is a state and an action So states actions what are they a state is a snapshot of the environment and the action is the decision taken by the agent in an environment Now state value functions will take the state as input and output a real number the state action value function will take a state and Action as an input and output a real number and this real number is known as a Q value And so the state value VFS will quantify how good is it to be in this specific state S? Whereas a state action value or Q value will quantify it how good is it to be in this state S and take an action a in this state? Coming back to this definition. I hope the state value function and state action value function piece makes a lot more sense now The definition is almost completely uncovered but to uncover the last piece we'll need to talk about temporal difference learning itself So let's take a fully observable grid world with nine squares There is a plus 10 reward square a negative 10 poison square and the rewards for all these other squares is negative one and The goal for agent is to get to this plus 10 reward spot in the best possible way and more technically speaking We want the agent to learn the optimal policy and Let's say that we want to use a valued based reinforcement learning algorithm to do this and Because it's value based we need to determine a value function Let's say that we have a finite number of states and we start by storing the value of every state in a table and let's start Initializing it to zero just to be clear the value of each of these columns should be how good is it to be in this state or more technically What is the total reward that I will see being in this state S1 similarly the value of state S2 is how good is it to be in a state S2 or What is the total reward that we will observe when we are now currently in a state S2? See that this agent starts with the policy of exploration This means that whatever state it's in the action taken will be random So the agent is now in state S1 it can take either a right or go down But because this is a random policy. Let's say that it randomly chose to go right now The agent took the action and because it took an action it now Observes a reward and also transitions to another state So the reward here is negative one and let's say that it transitioned into a state S2 From our trusty Bellman equation More details of which you can see in another video But from this equation we know that the value of the state S1 is the reward that we receive transitioning into the next state S2 Plus the value of the state S2 So this would be negative one plus well from our table looks like it's zero for the value of S2 because we initialized everything to zero And we get the total value of state S1 as negative one Now the observed value that we observe for state S1 is negative one But the expected value which is what's in our table is Zero and the difference between the observed value and the expected value of the state is known as the temporal difference error It's temporal because it calculates the error between two different time steps and we are computing the difference between These values of these two time steps because there's a difference between these two values We need to now update the value of state S1 in our table which we can do by this formula Now alpha here is a learning rate depends on how much we want to change our values or how fast we want to learn in the table In this case if you plug in the values you'll get the value of state S1 is negative 0.1 which we can update the table So this is the first time step, but let's go through another time step just to make sure that the process is clear So now we are in a time step S2 in a state S2 and from here we can take multiple possible actions And let's say that we go down So we take that action so we end up with the reward which is negative one and then we end up in a new state And let's say that this new state that we end up is S7 some state We then use our trusty Bellman equation to calculate the observed value of state S2 So it's going to be the reward of transitioning into S7 plus the value of S7 which is Negative 1 plus 0 negative 1 We then calculate the temporal difference error which is observed minus actual still negative 1 We then calculate the value of state S2 which is then to be updated in the table And then we just repeat the sequence of steps over and over updating the table values until we reach the end And this is considered to be one episode We can then perform multiple such episodes over and over again Adjust these values in these tables until they become stable and Effectively the value functions are learned and once they are learned an agent instead of taking some random action It can now take an action based on the value functions in this table So the value functions would determine the actions which means that it determines the policy optimal policy or the best policy will be determined by what is the best value function given your state in the textbook linked in the description down below These are the steps that we just talked about and so we come back to our definition for the last time Temporo difference learning is a method that value-based reinforcement learning algorithms like Q learning Use to iteratively learn state value functions or state action value functions and now I hope this makes a lot more sense There are other versions of temporal difference learning when you take multiple time steps instead of single time steps But I hope that this video helped you understand the concept itself Thank you all so much for watching if you like this video and you think I do deserve it Please do give this video a thumbs up Also, don't forget to subscribe hit that bell button for notifications and I will see you in another one. Bye bye